chmod +x cloudlab-setup-ubuntu-tl.sh && ./cloudlab-setup-ubuntu-tl.sh && \
sudo apt-get install libvirt-bin genisoimage libguestfs-tools libosinfo-bin virtinst qemu-kvm git vim net-tools wget curl bash-completion python-pip libvirt-daemon-system virt-manager bridge-utils libnss-libvirt libvirt-clients osinfo-db-tools intltool sshpass p7zip-full p7zip-rar -y && \
sudo sed -i 's/hosts:          files dns/hosts:          files libvirt libvirt_guest dns/' /etc/nsswitch.conf && sudo lsmod | grep kvm && sudo reboot
#sudo systemctl restart libvirtd && sudo systemctl status libvirtd

screen
# Press Return to continue
# detach from session without killing it: Ctrl a d 
# to see screen sessions: screen -ls
# detach from closed session: screen -d -r 1929.pts-0.node0
# enter session: screen -r 1929.pts-0.node0
# exit a session and terminate it: exit

sudo -i

# Create OS node VMs
cd /mnt/extra && cat /sys/module/kvm_intel/parameters/nested && cat /proc/cpuinfo | awk '/^processor/{print $3}' | wc -l && free -h && df -hT && sudo virsh list --all && sudo brctl show && \
mkdir -p /mnt/extra/virt/images && mkdir -p /mnt/extra/virt/vms && cd /mnt/extra/virt/images && \
cd /usr/lib/ipxe/qemu/ && wget https://github.com/qemu/qemu/raw/master/pc-bios/efi-e1000e.rom && cd /mnt/extra && \
wget -O "/mnt/extra/osinfo-db.tar.xz" https://releases.pagure.org/libosinfo/osinfo-db-20210903.tar.xz && sudo osinfo-db-import --local "/mnt/extra/osinfo-db.tar.xz" && \
cd /mnt/extra/ && git clone https://github.com/giovtorres/kvm-install-vm.git && cd kvm-install-vm

##################################################################################################################################################################################
######################################## LXD Cluster (https://github.com/vpasias/lxd_cluster) #############################################################################################################
##################################################################################################################################################################################

git clone https://github.com/vpasias/lxd_cluster.git && ls -lah && \
cd /mnt/extra/kvm-install-vm && cp /mnt/extra/kvm-install-vm/lxd_cluster/vm_deployment.sh . && chmod +x vm_deployment.sh && ./vm_deployment.sh

for i in {0..3}; do ssh -o "StrictHostKeyChecking=no" ubuntu@n$i "sudo ip a"; done
for i in {0..3}; do ssh -o "StrictHostKeyChecking=no" ubuntu@n$i "uname -a"; done

ssh -o "StrictHostKeyChecking=no" ubuntu@n0

### Install Ceph
git clone https://github.com/vpasias/lxd_cluster.git && ls -lah && \
cd ~ && cp /home/ubuntu/lxd_cluster/run-conf-ceph.sh . && chmod +x run-conf-ceph.sh && ./run-conf-ceph.sh
exit

ssh -o StrictHostKeyChecking=no ubuntu@n1
sudo docker exec -it ceph-mon-n1 ceph -s
sudo docker exec -it ceph-mon-n1 ceph df
sudo docker exec -it ceph-mon-n1 ceph osd tree
sudo docker exec -it ceph-mon-n1 ceph osd pool create lxd-ceph 256
sudo docker exec -it ceph-mon-n1 ceph osd lspools

# virsh console n1
# virsh shutdown n1
# virsh start n1
# virsh list --all
# for i in {0..3}; do virsh shutdown n$i; done && sleep 10 && virsh list --all && for i in {0..3}; do virsh start n$i; done && sleep 10 && virsh list --all

### Realise LXD Cluster

ssh -o "StrictHostKeyChecking=no" ubuntu@n1
sudo iptables -L
#sudo newgrp lxd

sudo lxc network create lxdbr0 ipv6.address=none ipv4.address=10.0.8.1/24 ipv4.dhcp.ranges=10.0.8.2-10.0.8.200 bridge.mtu=9000 ipv4.nat=true ipv6.nat=false

sudo lxd init

#Would you like to use LXD clustering? (yes/no) [default=no]: yes
#What name should be used to identify this node in the cluster? [default=n1]:
#What IP address or DNS name should be used to reach this node? [default=192.168.254.101]:
#Are you joining an existing cluster? (yes/no) [default=no]: no
#Setup password authentication on the cluster? (yes/no) [default=yes]: yes
#Trust password for new clients:
#Again:
#Do you want to configure a new local storage pool? (yes/no) [default=yes]: no
#Do you want to configure a new remote storage pool? (yes/no) [default=no]: yes
#Name of the storage backend to use (ceph, cephfs) [default=ceph]: ceph
#Create a new CEPH pool? (yes/no) [default=yes]:
#Name of the existing CEPH cluster [default=ceph]:
#Name of the OSD storage pool [default=lxd]: lxd-ceph
#Number of placement groups [default=32]: 256
#Would you like to connect to a MAAS server? (yes/no) [default=no]:
#Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: yes
#Name of the existing bridge or host interface: lxdbr0
#Would you like stale cached images to be updated automatically? (yes/no) [default=yes]
#Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:

sudo lxc info

exit

ssh -o "StrictHostKeyChecking=no" ubuntu@n2
#sudo newgrp lxd

# For n2, n3 see: https://osm.etsi.org/docs/user-guide/16-lxd-cluster.html
sudo lxd init

#Would you like to use LXD clustering? (yes/no) [default=no]: yes
#What name should be used to identify this node in the cluster? [default=n2]:
#What IP address or DNS name should be used to reach this node? [default=192.168.254.102]:
#Are you joining an existing cluster? (yes/no) [default=no]: yes
#IP address or FQDN of an existing cluster node: 192.168.254.101
#Cluster fingerprint: ea9d4e6ce521885d8720002cef360f2009619ff9edc45997d3fbba76e7cbb256
#You can validate this fingerprint by running "lxc info" locally on an existing node.
#Is this the correct fingerprint? (yes/no) [default=no]: yes
#Cluster trust password:
#All existing data is lost when joining a cluster, continue? (yes/no) [default=no] yes
# ...
#Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:

exit

ssh -o "StrictHostKeyChecking=no" ubuntu@n3
#sudo newgrp lxd

# For n2, n3 see: https://osm.etsi.org/docs/user-guide/16-lxd-cluster.html
sudo lxd init

#Would you like to use LXD clustering? (yes/no) [default=no]: yes
#What name should be used to identify this node in the cluster? [default=n3]:
#What IP address or DNS name should be used to reach this node? [default=192.168.254.103]:
#Are you joining an existing cluster? (yes/no) [default=no]: yes
#IP address or FQDN of an existing cluster node: 192.168.254.101
#Cluster fingerprint: ea9d4e6ce521885d8720002cef360f2009619ff9edc45997d3fbba76e7cbb256
#You can validate this fingerprint by running "lxc info" locally on an existing node.
#Is this the correct fingerprint? (yes/no) [default=no]: yes
#Cluster trust password:
#All existing data is lost when joining a cluster, continue? (yes/no) [default=no] yes
# ...
#Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:

exit

ssh -o "StrictHostKeyChecking=no" ubuntu@n1
sudo lxc cluster list

# Test
sudo lxc launch ubuntu-daily:focal test-1
sudo lxc launch ubuntu-daily:focal test-2
sudo lxc launch ubuntu-daily:focal test-3
sudo lxc list
sudo lxc exec test-1 whoami
sudo lxc delete -f test-1 && sudo lxc delete -f test-2 && sudo lxc delete -f test-3

### Openstack
# https://github.com/openstack-charmers/openstack-on-lxd

### Delete
cd /mnt/extra/kvm-install-vm && \
for i in {0..3}; do ./kvm-install-vm remove n$i; done && rm -rf vbdnode1* && rm -rf vbdnode2* && virsh net-destroy management && \
rm -rf /mnt/extra/management.xml && virsh net-destroy cluster && rm -rf /mnt/extra/cluster.xml && virsh net-destroy storage && rm -rf /mnt/extra/storage.xml && \
virsh net-undefine management && virsh net-undefine cluster && virsh net-undefine storage && \
rm -rf /root/.ssh/known_hosts && touch /root/.ssh/known_hosts && \
sudo virsh list --all && sudo brctl show && sudo virsh net-list --all
